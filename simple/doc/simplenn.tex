\documentclass[10pt,a4paper]{article}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amssymb}

\author{Raphael St\"abler}
\title{Simple Neural Network}
\date{\today{}}

\begin{document}

\begin{titlepage}
	\maketitle
	\thispagestyle{empty}
\end{titlepage}

\newpage

\section{Definition}
\subsection{Training examples}
We use the following $4 \times 3$ matrix as input for our training:
\begin{equation}
	X =
	\begin{bmatrix}
		0 & 0 & 1 \\
		0 & 1 & 1 \\
		1 & 0 & 1 \\
		1 & 1 & 1
	\end{bmatrix}
\end{equation}	
While this $4 \times 1$ matrix defines the desired outputs:
\begin{equation}
	Y =
	\begin{bmatrix}
		0 \\
		0 \\
		1 \\
		1
	\end{bmatrix}
\end{equation}
\subsection{Weight initialization}
Then we randomly initialize a $3 \times 1$ weight matrix:
\begin{equation}
	w =
	\begin{bmatrix}
		w_1 \\
		w_2 \\
		w_3 \\
	\end{bmatrix}
\end{equation}

\section{Forward propagation}
\subsection{Linear transformation}
Forward propagation through the network consists of a linear transformation of the type $\hat{y}: \mathbb{R}^3 \to \mathbb{R}^4$
\begin{equation}
	\hat{y} = Xw =
	\begin{bmatrix}
		0 & 0 & 1 \\
		0 & 1 & 1 \\
		1 & 0 & 1 \\
		1 & 1 & 1
	\end{bmatrix}
	\begin{bmatrix}
		w_1 \\
		w_2 \\
		w_3 \\
	\end{bmatrix}
	=
	\begin{bmatrix}
		w_3 \\
		w_2 + w_3 \\
		w_1 + w_3 \\
		w_1 + w_2 + w_3
	\end{bmatrix}
\end{equation}
\subsection{Activation function}
To confine the output between $0$ and $1$ we then apply the sigmoid function $y = sigmoid(\hat{y})$
\begin{equation}
	y = \frac{1}{1+e^{-\hat{y}}}
\end{equation}

\newpage

\section{Loss and back propagation}
\subsection{Delta}
We calculate a delta using the derivative of the sigmoid function and the loss given by $Y-y$
\begin{equation}
	\Delta{y} = (Y-y)\frac{dy}{d\hat{y}} = (Y-y)(y-y^2)
\end{equation}

\subsection{Weight update}
We now update our weights $w$:
\begin{equation}
	w = w + X^T\Delta{y}
\end{equation}
The same equation expanded:
\begin{equation}
	\begin{bmatrix}
		w_1 \\
		w_2 \\
		w_3 \\
	\end{bmatrix}
	=
	\begin{bmatrix}
		w_1 \\
		w_2 \\
		w_3 \\
	\end{bmatrix}
	+
	\begin{bmatrix}
		0 & 0 & 1 & 1 \\
		0 & 1 & 0 & 1 \\
		1 & 1 & 1 & 1
	\end{bmatrix}
	\begin{bmatrix}
		\Delta{y_1} \\
		\Delta{y_2} \\
		\Delta{y_3} \\
		\Delta{y_4}
	\end{bmatrix}
\end{equation}
Which ultimately means:
\begin{equation}
	w =
	\begin{bmatrix}
		w_1 + \Delta{y_3} + \Delta{y_4} \\
		w_2 + \Delta{y_2} + \Delta{y_4} \\
		w_3 + \Delta{y_1} + \Delta{y_2} + \Delta{y_3} + \Delta{y_4} \\
	\end{bmatrix}
\end{equation}

\newpage

\section{Example calculation}
\subsection{Initialize}
Set $X \in \mathbb{R}^{n \times 3}$
\begin{equation}
	X =
	\begin{bmatrix}
		0 & 0 & 1 \\
		\vdots & \vdots & \vdots \\
	\end{bmatrix}
\end{equation}	
Set $Y \in \mathbb{R}^{n \times 1}$
\begin{equation}
	Y =
	\begin{bmatrix}
		0 \\
		\vdots
	\end{bmatrix}
\end{equation}
Set $w \in \mathbb{R}^{3 \times 1}$
\begin{equation}
	w =
	\begin{bmatrix}
		\vdots \\
		0.4
	\end{bmatrix}
\end{equation}

\subsection{Forward propagation}
Caluclate $y$
\begin{equation}
	y =
	\begin{bmatrix}
		\frac{1}{1+e^{-0.4}} \\
		\vdots
	\end{bmatrix}
	\approx
	\begin{bmatrix}
		0.6 \\
		\vdots
	\end{bmatrix}
\end{equation}

\subsection{Loss and back propagation}
Calculate $\Delta{y}$
\begin{equation}
	\Delta{y} = (
	\begin{bmatrix}
		0 \\
		\vdots
	\end{bmatrix}
	-
	\begin{bmatrix}
		0.6 \\
		\vdots
	\end{bmatrix}
	)(
	\begin{bmatrix}
		0.6 \\
		\vdots
	\end{bmatrix}
	-
	\begin{bmatrix}
		0.6^2 \\
		\vdots
	\end{bmatrix}
	) =
	\begin{bmatrix}
		-0.144 \\
		\vdots
	\end{bmatrix}
\end{equation}
Update weight $w$
\begin{equation}
	w =
	\begin{bmatrix}
		\vdots \\
		0.4
	\end{bmatrix}
	+
	\begin{bmatrix}
		0 & \ldots \\
		0 & \ldots \\
		1 & \ldots
	\end{bmatrix}
	\begin{bmatrix}
		-0.144 \\
		\vdots
	\end{bmatrix}
	\leq
	\begin{bmatrix}
		\vdots \\
		0.256
	\end{bmatrix}
\end{equation}

\subsection{Another prediction}
Another forward pass gets is closer to the desired output:
\begin{equation}
	y =
	\begin{bmatrix}
		\frac{1}{1+e^{-0.256}} \\
		\vdots
	\end{bmatrix}
	\approx
	\begin{bmatrix}
		0.56 \\
		\vdots
	\end{bmatrix}
\end{equation}
\end{document}