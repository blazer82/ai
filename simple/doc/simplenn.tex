\documentclass[10pt,a4paper]{article}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amssymb}

\author{Raphael St\"abler}
\title{Simple Neural Network}
\date{\today{}}

\begin{document}

\begin{titlepage}
	\maketitle
	\thispagestyle{empty}
\end{titlepage}

\newpage

\section{Definition}
\subsection{Training examples}
We use the following $4 \times 3$ matrix as input for our training:
\begin{equation}
	X =
	\begin{bmatrix}
		0 & 0 & 1 \\
		0 & 1 & 1 \\
		1 & 0 & 1 \\
		1 & 1 & 1
	\end{bmatrix}
\end{equation}	
While this $4 \times 1$ matrix defines the desired outputs:
\begin{equation}
	Y =
	\begin{bmatrix}
		0 \\
		0 \\
		1 \\
		1
	\end{bmatrix}
\end{equation}
\subsection{Weight initialization}
Then we randomly initialize a $3 \times 1$ weight matrix:
\begin{equation}
	w =
	\begin{bmatrix}
		w_1 \\
		w_2 \\
		w_3 \\
	\end{bmatrix}
\end{equation}

\section{Forward propagation}
\subsection{Linear transformation}
Forward propagation through the network consists of a linear transformation of the type $\hat{y}: \mathbb{R}^3 \to \mathbb{R}^4$
\begin{equation}
	\hat{y} = Xw =
	\begin{bmatrix}
		0 & 0 & 1 \\
		0 & 1 & 1 \\
		1 & 0 & 1 \\
		1 & 1 & 1
	\end{bmatrix}
	\begin{bmatrix}
		w_1 \\
		w_2 \\
		w_3 \\
	\end{bmatrix}
	=
	\begin{bmatrix}
		w_3 \\
		w_2 + w_3 \\
		w_1 + w_3 \\
		w_1 + w_2 + w_3
	\end{bmatrix}
\end{equation}
\subsection{Activation function}
To confine the output between $0$ and $1$ we then apply the sigmoid function $y = sigmoid(\hat{y})$
\begin{equation}
	y = \frac{1}{1+e^{-\hat{y}}}
\end{equation}

\newpage

\section{Loss and backpropagation}
\subsection{Delta}
We calculate a delta using the derivative of the sigmoid function and the loss given by $Y-y$
\begin{equation}
	\Delta{y} = (Y-y)\frac{dy}{d\hat{y}} = (Y-y)(y-y^2)
\end{equation}

\subsection{Weight update}
We now update our weights $w$:
\begin{equation}
	w = w + X^T\Delta{y}
\end{equation}
The same equation expanded:
\begin{equation}
	\begin{bmatrix}
		w_1 \\
		w_2 \\
		w_3 \\
	\end{bmatrix}
	=
	\begin{bmatrix}
		w_1 \\
		w_2 \\
		w_3 \\
	\end{bmatrix}
	+
	\begin{bmatrix}
		0 & 0 & 1 & 1 \\
		0 & 1 & 0 & 1 \\
		1 & 1 & 1 & 1
	\end{bmatrix}
	\begin{bmatrix}
		\Delta{y_1} \\
		\Delta{y_2} \\
		\Delta{y_3} \\
		\Delta{y_4}
	\end{bmatrix}
\end{equation}
Which ultimately means:
\begin{equation}
	w =
	\begin{bmatrix}
		w_1 + \Delta{y_3} + \Delta{y_4} \\
		w_2 + \Delta{y_2} + \Delta{y_4} \\
		w_3 + \Delta{y_1} + \Delta{y_2} + \Delta{y_3} + \Delta{y_4} \\
	\end{bmatrix}
\end{equation}

\newpage

\section{Calculation example}
\subsection{Initialize}
Set $X \in \mathbb{R}^{n \times 3}$
\begin{equation}
	X =
	\begin{bmatrix}
		0 & 0 & 1 \\
		\vdots & \vdots & \vdots \\
	\end{bmatrix}
\end{equation}	
Set $Y \in \mathbb{R}^{n \times 1}$
\begin{equation}
	Y =
	\begin{bmatrix}
		0 \\
		\vdots
	\end{bmatrix}
\end{equation}
Set $w \in \mathbb{R}^{3 \times 1}$
\begin{equation}
	w =
	\begin{bmatrix}
		\vdots \\
		0.4
	\end{bmatrix}
\end{equation}

\subsection{Forward propagation}
Caluclate $y$
\begin{equation}
	y =
	\begin{bmatrix}
		\frac{1}{1+e^{-0.4}} \\
		\vdots
	\end{bmatrix}
	\approx
	\begin{bmatrix}
		0.6 \\
		\vdots
	\end{bmatrix}
\end{equation}

\subsection{Loss and backpropagation}
Calculate $\Delta{y}$
\begin{equation}
	\Delta{y} = (
	\begin{bmatrix}
		0 \\
		\vdots
	\end{bmatrix}
	-
	\begin{bmatrix}
		0.6 \\
		\vdots
	\end{bmatrix}
	)(
	\begin{bmatrix}
		0.6 \\
		\vdots
	\end{bmatrix}
	-
	\begin{bmatrix}
		0.6^2 \\
		\vdots
	\end{bmatrix}
	) =
	\begin{bmatrix}
		-0.144 \\
		\vdots
	\end{bmatrix}
\end{equation}
Update weight $w$
\begin{equation}
	w =
	\begin{bmatrix}
		\vdots \\
		0.4
	\end{bmatrix}
	+
	\begin{bmatrix}
		0 & \ldots \\
		0 & \ldots \\
		1 & \ldots
	\end{bmatrix}
	\begin{bmatrix}
		-0.144 \\
		\vdots
	\end{bmatrix}
	\leq
	\begin{bmatrix}
		\vdots \\
		0.256
	\end{bmatrix}
\end{equation}

\subsection{Another prediction}
Another forward pass gets us closer to the desired output:
\begin{equation}
	y =
	\begin{bmatrix}
		\frac{1}{1+e^{-0.256}} \\
		\vdots
	\end{bmatrix}
	\approx
	\begin{bmatrix}
		0.56 \\
		\vdots
	\end{bmatrix}
\end{equation}

\newpage

\section{Multiple layers}
When dealing with non-linear problems we would need to insert at least one additional layer between the input and the output layer.
In our example where $X$ is of shape $\mathbb{R}^{n \times 3}$ and Y is of shape $\mathbb{R}^{n \times 1}$ out weight matrices need to satisfy the following requirements:
\begin{equation}
	w^{(1)} \in \mathbb{R}^{3 \times m},
	w^{(2)} \in \mathbb{R}^{m \times 1}
\end{equation}

\subsection{Hidden layer}
Let $m=4$:
\begin{equation}
	w^{(1)} =
	\begin{bmatrix}
		w^{(1)}_{11} & w^{(1)}_{12} & w^{(1)}_{13} & w^{(1)}_{14} \\
		w^{(1)}_{21} & w^{(1)}_{22} & w^{(1)}_{23} & w^{(1)}_{24} \\
		w^{(1)}_{31} & w^{(1)}_{32} & w^{(1)}_{33} & w^{(1)}_{34}
	\end{bmatrix},
	w^{(2)} =
	\begin{bmatrix}
		w^{(2)}_{1} \\
		w^{(2)}_{2} \\
		w^{(2)}_{3} \\
		w^{(2)}_{4}
	\end{bmatrix}
\end{equation}

\subsection{Forward propagation}
Our forward propagation is now a set of linear transformations. One for the hidden layer:
\begin{equation}
	h = sigmoid(Xw^{(1)})
\end{equation}
And one for the output layer:
\begin{equation}
	y = sigmoid(hw^{(2)})
\end{equation}
The whole forward pass looks like this:
\begin{equation}
	y = sigmoid(sigmoid(
	\begin{bmatrix}
		0 & 0 & 1 \\
		0 & 1 & 1 \\
		1 & 0 & 1 \\
		1 & 1 & 1
	\end{bmatrix}
	\begin{bmatrix}
		w^{(1)}_{11} & w^{(1)}_{12} & w^{(1)}_{13} & w^{(1)}_{14} \\
		w^{(1)}_{21} & w^{(1)}_{22} & w^{(1)}_{23} & w^{(1)}_{24} \\
		w^{(1)}_{31} & w^{(1)}_{32} & w^{(1)}_{33} & w^{(1)}_{34}
	\end{bmatrix}
	)
	\begin{bmatrix}
		w^{(2)}_{1} \\
		w^{(2)}_{2} \\
		w^{(2)}_{3} \\
		w^{(2)}_{4}
	\end{bmatrix}
	)
\end{equation}

\newpage

\subsection{Loss and backpropagation}
\subsubsection{Delta}
$\Delta{y}$ stays unchanged:
\begin{equation}
	\Delta{y} = (Y-y)(y-y^2)
\end{equation}
We now need to establish a loss $\phi_h \in \mathbb{R}^{4 \times 4}$ for the hidden layer:
\begin{equation}
	\phi_h = \Delta{y}w^{(2)T}
\end{equation}
Which expands to:
\begin{equation}
	\phi_h =
	\begin{bmatrix}
		\Delta{y_1} \\
		\Delta{y_2} \\
		\Delta{y_3} \\
		\Delta{y_4}
	\end{bmatrix}
	\begin{bmatrix}
		w^{(2)}_{1} & w^{(2)}_{2} & w^{(2)}_{3} & w^{(2)}_{4}
	\end{bmatrix}
\end{equation}
There's a new delta $\Delta{h} \in \mathbb{R}^{4 \times 4}$ for our hidden layer:
\begin{equation}
	\Delta{h} = \phi_h (h-h^2)
\end{equation}

\subsubsection{Weight update}
$w^{(1)}$ is updated as follows:
\begin{equation}
	w^{(1)} = X^T\Delta{h}
\end{equation}
And $w^{(2)}$:
\begin{equation}
	w^{(2)} = h^T\Delta{y}
\end{equation}

\end{document}